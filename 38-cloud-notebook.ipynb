{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception('GPU not available.')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('gpu')\n",
    "\n",
    "SEED = 42\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f'Device: {torch.cuda.get_device_name(0)} | Seed: {SEED}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_FOLDER = 'data'\n",
    "SCRIPT_FOLDER = 'scripts'\n",
    "\n",
    "archive_path = os.path.join(DATA_FOLDER, 'archive.zip')\n",
    "CLOUD_TEST_PATH = os.path.join(DATA_FOLDER, '38-Cloud_test')\n",
    "CLOUD_TRAIN_PATH = os.path.join(DATA_FOLDER, '38-Cloud_training')\n",
    "\n",
    "# flag to check if archive.zip has already been unzipped\n",
    "is_archive_unzipped = os.path.exists(CLOUD_TEST_PATH) and os.path.exists(CLOUD_TRAIN_PATH)\n",
    "\n",
    "# raise error if archive.zip has not been found and it has not been unzipped yet\n",
    "if not os.path.exists(archive_path) and not is_archive_unzipped:\n",
    "    raise Exception('Error: archive.zip is missing from data folder')\n",
    "\n",
    "if is_archive_unzipped:\n",
    "    print('Archive already unzipped.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "if (is_archive_unzipped == False):\n",
    "    print('Unzipping archive...')\n",
    "    !unzip data/archive.zip -d data\n",
    "    print('Archive Unzipped!')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists(archive_path) and is_archive_unzipped:\n",
    "    !rm data/archive.zip\n",
    "    print('Archive Removed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('data/38-Cloud_test/test_gt'):\n",
    "    print('Building test gt...')\n",
    "    !python3 scripts/build_test_gt.py    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('data/testing_patches_38-cloud_nonempty.csv'):\n",
    "    print('Building test patches...')\n",
    "    !python3 scripts/build_nonempty.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patches_name = 'training_patches_38-Cloud.csv'\n",
    "test_patches_name = 'test_patches_38-Cloud.csv'\n",
    "\n",
    "df_patches_train = pd.read_csv(os.path.join(CLOUD_TRAIN_PATH, train_patches_name))  # patches from 38-Cloud_training\n",
    "df_patches_train['type'] = 'train'\n",
    "df_patches_test = pd.read_csv(os.path.join(CLOUD_TEST_PATH, test_patches_name))     # patches from 38-Cloud_test\n",
    "df_patches_test['type'] = 'test'\n",
    "\n",
    "df_patches=pd.concat([df_patches_train,df_patches_test]).reset_index(drop=True)     # concatenate the two dataframes\n",
    "\n",
    "print(f'Patches from train: {len(df_patches_train)} | Patches from test: {len(df_patches_test)}')\n",
    "#df_patches.head()\n",
    "\n",
    "tr_non_empty_patches_name = 'training_patches_38-cloud_nonempty.csv'\n",
    "df_patches_train_non_empty = pd.read_csv(os.path.join(DATA_FOLDER, tr_non_empty_patches_name))\n",
    "df_patches_train_non_empty['type'] = 'train'\n",
    "print (f'Number of non-empty training patches: {len(df_patches_train_non_empty)}')\n",
    "\n",
    "ts_non_empty_patches_name = 'testing_patches_38-cloud_nonempty.csv'\n",
    "df_patches_test_non_empty = pd.read_csv(os.path.join(DATA_FOLDER, ts_non_empty_patches_name))\n",
    "df_patches_test_non_empty['type'] = 'test'\n",
    "print (f'Number of non-empty testing patches: {len(df_patches_test_non_empty)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, Subset, random_split\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "class Cloud38Dataset(Dataset):\n",
    "    def __init__(self, patches_df, transform=None):\n",
    "        super().__init__()        \n",
    "        self.transform = transform\n",
    "        self.patches = [self.map_patches(f, t) for f, t in zip(patches_df['name'], patches_df['type'])]           \n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    def map_patches(self, file_name, type_p):\n",
    "\n",
    "        folder_path = Path(CLOUD_TEST_PATH) if type_p == 'test' else Path(CLOUD_TRAIN_PATH)\n",
    "\n",
    "        channels = ['red', 'green', 'blue', 'nir', 'gt']\n",
    "        paths = {}\n",
    "\n",
    "        for i in channels:\n",
    "            path = folder_path / Path(f'{type_p}_{i}') / Path(f'{i}_' + file_name + '.TIF')\n",
    "            if not os.path.exists(path):\n",
    "                raise Exception(f'Error: {path} does not exist' )\n",
    "            paths[i] = path   \n",
    "\n",
    "        return paths\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "\n",
    "    def to_array(self, i, invert=False, include_nir=False):\n",
    "        rgb = np.stack([\n",
    "            np.array(Image.open(self.patches[i]['red'])),\n",
    "            np.array(Image.open(self.patches[i]['green'])),\n",
    "            np.array(Image.open(self.patches[i]['blue'])),\n",
    "        ], axis=2)\n",
    "\n",
    "        if include_nir:\n",
    "            nir = np.expand_dims(np.array(Image.open(self.patches[i]['nir'])), axis=2)\n",
    "            rgb = np.concatenate([rgb, nir], axis=2)\n",
    "\n",
    "        if invert: # Torch wants the color channel first, so Color x Height x Width\n",
    "            rgb = rgb.transpose((2, 0, 1))\n",
    "        \n",
    "        return rgb / np.iinfo(rgb.dtype).max # images are uint16, so max is 65535\n",
    "          \n",
    "    \n",
    "    def to_mask(self, i, add_dims=False):\n",
    "        mask = np.array(Image.open(self.patches[i]['gt']))\n",
    "        mask = np.where(mask == 255, 1, 0)\n",
    "\n",
    "        if add_dims:\n",
    "            return np.expand_dims(mask, 0)\n",
    "        else:\n",
    "            return mask\n",
    "        \n",
    "    def __getitem__(self, i):         \n",
    "        img = self.to_array(i, invert=False)\n",
    "        mask = self.to_mask(i)\n",
    "\n",
    "        #print(f'Image shape: {img.shape} | Mask shape: {mask.shape}')        \n",
    "\n",
    "        # Apply transformations if defined\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=img, mask=mask)\n",
    "            img = augmentations['image']\n",
    "            mask = augmentations['mask']\n",
    "            # Return type is Tensor\n",
    "\n",
    "        else:        \n",
    "            img = torch.tensor(img.transpose(2, 0, 1), dtype=torch.float32)\n",
    "            mask = torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "        return img, mask        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup custom cmap for mask\n",
    "custom_cmap = plt.colormaps['viridis']  # We change the default colormap to make sure that 0 is purple and 1 is yellow\n",
    "custom_cmap.set_under('purple')         # Wven when the mask has only 0s or 1s\n",
    "custom_cmap.set_over('yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load of training dataset\n",
    "data_train = Cloud38Dataset(df_patches_train_non_empty)     # We use non empty patches for more efficient training\n",
    "print(f'Loaded Training data   | Length: {len(data_train)}')\n",
    "\n",
    "data_test_val = Cloud38Dataset(df_patches_test_non_empty, transform=None)\n",
    "dtv_len2 = len(data_test_val) // 2\n",
    "data_test, data_val = random_split(data_test_val, [dtv_len2 + 1, dtv_len2])\n",
    "print(f'Loaded Testing data    | Length: {len(data_test)}')\n",
    "print(f'Loaded Validation data | Length: {len(data_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = np.random.randint(0, len(data_train))\n",
    "test_index = np.random.randint(0, len(data_test))\n",
    "val_index = np.random.randint(0, len(data_val))\n",
    "\n",
    "idxs = {'train': train_index, 'test': test_index, 'val': val_index}\n",
    "\n",
    "fig, ax = plt.subplots(1, 6, figsize=(20, 15))\n",
    "\n",
    "x = 0\n",
    "for i, j in enumerate(idxs):\n",
    "    ax[i*2].imshow(data_train.to_array(idxs[j]))\n",
    "    ax[i*2].set_title(f'{j.capitalize()} image')\n",
    "    ax[i*2].set_axis_off()\n",
    "    ax[i*2+1].imshow(data_train.to_mask(idxs[j]), cmap=custom_cmap, vmin=0, vmax=1)\n",
    "    ax[i*2+1].set_title(f'{j.capitalize()} mask')\n",
    "    ax[i*2+1].set_axis_off()\n",
    "\n",
    "print(data_train.to_array(train_index))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "IMAGE_SIZE = 384\n",
    "\n",
    "if False: # Temporary flag to disable mean and std computation\n",
    "    loader_train = DataLoader(data_train, batch_size=32)\n",
    "\n",
    "    pixel_sum = torch.tensor([0.0, 0.0, 0.0])\n",
    "    pixel_sum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
    "\n",
    "    for x, y in loader_train:\n",
    "        pixel_sum += x.sum(axis=(0, 2, 3))\n",
    "        pixel_sum_sq += (x ** 2).sum(axis=(0, 2, 3))\n",
    "\n",
    "    count = len(data_train) * IMAGE_SIZE * IMAGE_SIZE\n",
    "    mean = pixel_sum / count\n",
    "    var = (pixel_sum_sq / count) - (mean ** 2)\n",
    "    std = torch.sqrt(var)\n",
    "\n",
    "else:\n",
    "    mean = torch.tensor([0.2400, 0.2366, 0.2508])\n",
    "    std = torch.tensor([0.1451, 0.1393, 0.1443])\n",
    "\n",
    "print(f'Computed mean and std for the dataset:\\n Mean: {mean} | Std: {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters for training\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = device\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 3\n",
    "IMAGE_RESIZE = 384\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL= False # True if you want to load a previous model from the checkpoint.pth.tar file\n",
    "MODELS_FOLDER = 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transform = A.Compose([    \n",
    "    A.Resize(height=IMAGE_RESIZE, width=IMAGE_RESIZE),\n",
    "    A.Rotate(limit=35, p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.1),\n",
    "    A.Normalize(\n",
    "    mean=mean.tolist(),   # mean=mean.tolist() | mean=[0.0, 0.0, 0.0]\n",
    "    std=std.tolist(),     # std=std.tolist()   | std=[1.0, 1.0, 1.0]\n",
    "    max_pixel_value=1,    \n",
    "    ),\n",
    "    ToTensorV2(),       \n",
    "])\n",
    "\n",
    "val_transforms = A.Compose([    \n",
    "    A.Resize(height=IMAGE_RESIZE, width=IMAGE_RESIZE),\n",
    "    A.Normalize(\n",
    "    mean=mean.tolist(),  \n",
    "    std=std.tolist(),   \n",
    "    max_pixel_value=1,    \n",
    "    ),\n",
    "    ToTensorV2(),       \n",
    "])\n",
    "\n",
    "if False: # True for training with augmentations\n",
    "    sub_train = Cloud38Dataset(df_patches_train_non_empty, transform=train_transform)\n",
    "    sub_val = Cloud38Dataset(df_patches_test, transform=val_transforms)\n",
    "else:\n",
    "    sub_train = Cloud38Dataset(df_patches_train_non_empty, transform=None)\n",
    "    sub_val = Cloud38Dataset(df_patches_test, transform=None)\n",
    "\n",
    "sub_train = Subset(sub_train, np.random.randint(0, len(data_train), 10),)\n",
    "sub_val = Subset(sub_val, np.random.randint(0, len(data_val), 10))\n",
    "\n",
    "print(f'Length of sub_train: {len(sub_train)} | Length of sub_val: {len(sub_val)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the output of normalization\n",
    "dl_normalized = DataLoader(sub_train, shuffle=True)\n",
    "for x, y in dl_normalized:\n",
    "    z = x[0].permute(1, 2, 0)\n",
    "    z = (z - z.min()) / (z.max() - z.min())\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "    ax[0].imshow(z)    \n",
    "    ax[1].imshow(y[0], cmap=custom_cmap, vmin=0, vmax=1)   \n",
    "    print(f'Normalized image sits between {x.min()} and {x.max()}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from model import UNET\n",
    "from tqdm import tqdm\n",
    "from utils import (\n",
    "    load_checkpoint, \n",
    "    save_checkpoint,    \n",
    "    check_accuracy,\n",
    "    save_predictions_as_imgs,\n",
    ")\n",
    "\n",
    "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
    "    loop = tqdm(loader)\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device=DEVICE)\n",
    "        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
    "\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = model(data)\n",
    "            print(f'Predictions shape: {predictions.shape} | Targets shape: {targets.shape}')\n",
    "            loss = loss_fn(predictions, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # update tqdm loop\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "def main():    \n",
    "    model = UNET(in_channels=3, out_channels=1).to(DEVICE)\n",
    "    loss_fn = nn.BCEWithLogitsLoss() # Binary Cross Entropy with Logits\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=sub_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,        \n",
    "        pin_memory=PIN_MEMORY,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        dataset=sub_val,\n",
    "        batch_size=1,\n",
    "        shuffle=False,        \n",
    "        pin_memory=PIN_MEMORY,\n",
    "    )\n",
    "\n",
    "    model_path = os.path.join(MODELS_FOLDER, 'unet.pth.tar')\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(torch.load(model_path), model)               \n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_fn(train_loader, model, optimizer, loss_fn, scaler)         \n",
    "\n",
    "        # save model\n",
    "        checkpoint = {\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }\n",
    "        save_checkpoint(checkpoint, filename=model_path)\n",
    "        \n",
    "        # check acc\n",
    "        check_accuracy(val_loader, model, device=DEVICE)\n",
    "        \n",
    "        #save\n",
    "        save_predictions_as_imgs(\n",
    "            val_loader, model, folder='imgs/', device=DEVICE\n",
    "        )\n",
    "\n",
    "        print(f\"Epoch {epoch} completed!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_img_p = 'imgs/pred_9.png'\n",
    "pred_img = Image.open(pred_img_p)\n",
    "pred_img = np.array(pred_img)\n",
    "plt.imshow(pred_img)\n",
    "#pred_img = np.where(pred_img == 255, 1, 0)\n",
    "#plt.imshow(np.where(pred_img[:, :, 0] == 255, 1, 0), cmap=custom_cmap, vmin=0, vmax=1)\n",
    "print(pred_img[:, :, 0])\n",
    "\n",
    "# TODO: Predictions are made on three channels and done give a bianry mask\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
